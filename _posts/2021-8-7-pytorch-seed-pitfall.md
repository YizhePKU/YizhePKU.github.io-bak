# Pitfall of random seed with multi-process training

Quoting from [PyTorch Doc](https://pytorch.org/docs/stable/data.html#data-loading-randomness):

> #### Randomness in multi-process data loading
>
> By default, each worker will have its PyTorch seed set to `base_seed + worker_id`, where `base_seed` is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified `generator`. However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See [this section](https://pytorch.org/docs/stable/notes/faq.html#dataloader-workers-random-seed) in FAQ.).
>
> In `worker_init_fn`, you may access the PyTorch seed set for each worker with either [`torch.utils.data.get_worker_info().seed`](https://pytorch.org/docs/stable/data.html#torch.utils.data.get_worker_info) or [`torch.initial_seed()`](https://pytorch.org/docs/stable/generated/torch.initial_seed.html#torch.initial_seed), and use it to seed other libraries before data loading.



In particular, if you're mixing NumPy and PyTorch when doing multi-process training, you need to set NumPy's seed for each process manually, like this:

```python
worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2**32 + id)
```



Here is the [Original Reddit post](https://www.reddit.com/r/MachineLearning/comments/mocpgj/p_using_pytorch_numpy_a_bug_that_plagues/).

